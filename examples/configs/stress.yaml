# Ensure number of examples is divisible by number of workers
# until https://github.com/NVIDIA/reinforcer/issues/125
#
# ??? = these are populated by the script (no need to specify)

generation:
  backend: "vllm" # vllm || hf (not HF is slow and only one instance, fully sharded is brought up)
  max_new_tokens: ???  # Max generation tokens (limited by max_model_len - prompt_length)
  temperature: ???
  top_p: ???
  top_k: ???
  model_name: "meta-llama/Llama-3.2-1B-Instruct"
  vllm_cfg:
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.6
    max_model_len: 8192  # Max length model can process (includes input + generation tokens) 

policy:
  model_name: ${generation.model_name}
  max_logprob_batch_size: 4  # Consider changing this if OOM-ing
  precision: "bfloat16"

  generation_batch_size: ???  # Only used if generation.backend == "hf"
  generation: ${generation}  # Only used if generation.backend == "hf"

data:
  numbers:
    # The total number of elements should be divisible by the number of workers which is determined by tensor_parallel_size and gpus_per_node/num_nodes.
    # Each element in the list specifies how long to make the text prompt. Actual input_seq_len may be longer or shorter.
    # Example: 5 -> "0 1 2 3 4"
    prompt_lengths: [8, 8, 32, 32, 64, 64, 64, 64,
                     128, 128, 128, 128, 256, 256, 256, 256,]
                     #512, 512, 512, 512, 1024, 1024, 2048, 2048]
  random:
    # The total number of elements should be divisible by the number of workers which is determined by tensor_parallel_size and gpus_per_node/num_nodes.
    # Each element in the list specifies how long to make the text prompt. Actual input_seq_len may be longer or shorter.
    # Example: 5 -> "f;t #"
    prompt_lengths: [8, 8, 32, 32, 64, 64, 64, 64,
                     128, 128, 128, 128, 256, 256, 256, 256,]
                     #512, 512, 512, 512, 1024, 1024, 2048, 2048]
  literal:
    # Literal strings to be used as prompts. Also supports message_log format:
    # Example message_log:
    # - [{"role": "user", "content": "<|begin_of_text|> Hello world! <|eot_id|>"}]
    - "What's the weather in Tokyo?"
    - "When was NVIDIA founded?"
    - "System: Be a helpful assistant. User: What is 1+1? Assistant: It's 2! User: Are you sure? Assistant: Yes, I'm sure!"
    - "Do re me fa so la ti"
    - "Continue the pattern: Volta Amphere Hopper..."
    - "Are you conscious?"
    - "Have we reached AGI?"
    - "Jingle bells, jingle bells, jingle"

cluster:
  gpus_per_node: 8
  num_nodes: 1
