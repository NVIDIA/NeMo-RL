# Evaluation Configuration
eval:
  seed: 42
  num_repeats: 1

generation:
  backend: "vllm" # only vllm is supported for evaluation
  max_new_tokens: ${generation.vllm_cfg.max_model_len}
  temperature: 0.0
  top_p: 1.0
  top_k: -1 # -1 means disable
  num_prompts_per_step: -1 # -1 means pass all prompts at once
  model_name: "Qwen/Qwen2.5-Math-1.5B-Instruct"
  stop_token_ids: null
  stop_strings: null
  vllm_cfg:
    precision: "bfloat16"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    max_model_len: 4096

data:
  train:
    shuffle: False
    seed: 420 # unused if shuffle is False
    jsonl_path: ...

    # checks if it's too long on init
    filter_long_samples: false
    drop_last: True
  
  max_input_seq_length: ${generation.vllm_cfg.max_model_len} # upper bound, real truncation occurs at vllm.max_model_len

tokenizer:
  name: ${generation.model_name} ## specify if you'd like to use a tokenizer different from the model's default

save_dir: eval_results

env:
  math:
    enable: true
    num_workers: 8
  llm_judge_async: # Configuration for the LlmJudgeAsyncEnvironment
    enable: false
    num_workers: 2 # Number of parallel judge workers
    model_name: "Qwen/Qwen2.5-32B-Instruct" # Judge model
    tensor_parallel_size: 4 # TP size for the judge model
    gpu_memory_utilization: 0.85 # For judge model
    max_model_len: 16384 # Max sequence length for the judge model
    temperature: 0.0 # Judge temperature (usually lower for more deterministic judging)
    max_tokens: 512  # Max tokens for the judge's evaluation
    stop: null # Stop strings for the judge's evaluation
    max_concurrency: 16 # Maximum concurrent step calls for the environment actor (default: 16)

cluster:
  gpus_per_node: 1
  num_nodes: 1

# don't change this, hack so we can use the train validate function
master_config:
  logger:
    num_val_samples_to_print: 2
  grpo:
    max_rollout_turns: 1
    # not really used
    max_val_samples: 10000000000000000000000
    val_batch_size: 1
    # not really used
  policy:
    max_total_sequence_length: ${generation.vllm_cfg.max_model_len}
